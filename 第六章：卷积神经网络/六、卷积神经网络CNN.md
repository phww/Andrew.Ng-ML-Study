## 六、卷积神经网络/CNN

有监督的神经网络——CNN

- 一些模式太小
- 同样的模式会出现在不同的区域
- 下采样不会改变目标

无监督的神经网络——GAN和自动编码器（Autoencoder）

[参考视频：吴恩达深度学习第三课-卷积神经网络week1-2](https://www.bilibili.com/video/BV1F4411y7o7?p=1)

[参考博客：知乎：CNN](https://www.zhihu.com/question/52668301/answer/194998098?utm_source=hot_content_share&utm_medium=all)

### 基础知识

- 从边缘检测说起
  - 如下图所示，将一个图片的灰度矩阵和一个特殊的矩阵进行一种特殊的操作——卷积。就可以得到原始图片的垂直边缘
  - ![垂直边缘检测](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%9E%82%E7%9B%B4%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.png)

  - 图中第二个特殊矩阵的正式的名称为过滤器（filter）或核（kernel）。原始6x6图片和3x3的过滤器进行了卷积操作，从而获得了一个4x4的图片。可认为生成的图片的中间部分就是原始图片的垂直边缘，虽然看起来这个边缘很宽，但是这是因为原始图片只是6x6的大小，如果是1000x1000的大小，中间的分割线就看着不宽了

- 什么是卷积操作？

  - 百度百科：在泛函分析中，卷积、旋积或摺积(英语：Convolution)是通过两个函数f 和g 生成第三个函数的一种数学算子，表征函数f 与g经过**翻转和平移**的重叠部分函数值乘积对重叠长度的积分。

  - 直观的gif

    - ![卷积神经网络动图](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8A%A8%E5%9B%BE.gif)
    - 与前向传播对比，可以发现：第一层的输入神经元（像素点）并**不全部**参与输出神经元的计算，这是卷积神经网络和普通的神经网络的一大区别。因此前向传播的神经网络又被称为**全连接神经网络** 
    - ![CNN动图](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/CNN%E5%8A%A8%E5%9B%BE.gif)

    - 在前向传播神经网络中，如果处理图像，会先将图像的特征（像素点）转换为一维的数组。而在CNN中，会更直观的在图像原本的排列情况下进行计算，如上图所示。所以说CNN很适合**处理图像信息**

- 什么是过滤器/卷积核？

  - 用来与原始图像进行卷积的特殊矩阵，即卷积核似乎和前向传播网络中的每一层的**参数（权重）**类似。只是不再为每个输出神经元配备一组权重。而是固定一个卷积核权重矩阵，用卷积的方法去 处理所有输入神经元。所以很直观的可以发现：**CNN的所需要确定的权重序列远远小于前向传播网络**
  - ![卷积核](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%8D%B7%E7%A7%AF%E6%A0%B8.jpg)
  - 需要什么样的卷积核？
    - 一些卷积核内部的权重是特殊构造的，这些卷积核可以完成一些特殊的特征提取工作。比如前文提到的第一个卷积核就可以提取出垂直的边缘。比如Sobel filter、scharr filter等特殊的卷积核
    - 更一般的，正如前向传播一样，可以随机初始化卷积核中的权重。然后用后向传播的方法，让CNN自己学习提取一些简单的特征

- 填充(Padding)

  - 在最开始的例子中，输入了6x6大小的图片，而卷积核是3x3的。最后输出了4x4大小的图片
    - 事实上，如果输入$nxn$大小的图片，然后用$fxf$的卷积核卷积。得到的结果会是$(n-f+1)x(n-f+1)$大小的图片
  - 上述过程有两个问题
    - 输出的图片与原始图片大小不一样！！
    - 边缘的神经元（像素点）在卷积过程中只被使用过一次，造成的结果就是原始图像边缘的信息被丢失了
  - 因此可以用0填充原始图片周围的空间，使输出图片与原始图片大小一样的同时也保证了边缘的像素点被充分卷积
  - ![padding](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/padding.png)
  - 如何填充？
    - 如果设填充的维度为p，则要使原始图像和输出图像大小一样，就要满足$n+2p-f+1= n$。所以填充的维度$p=\frac{f-1}2$,对于例子中3x3的卷积核，需要填充的维度p = 1
  - 必须要填充吗？
    - 事实上有两种卷积方式，即Valid Convolutions和Same Convolutions
    - Valid Convolutions，代表不用填充。即$nxn * fxf\to(n-f+1)x(n-f+1)$
    - Same Convolutions,代表填充使输出与输入图片大小一致。即$(n+2p)x(n+2p)*fxf\to nxn$
  - $p=\frac{f-1}2$
    - 可以发现当卷积核的大小f为偶数时，p无法得到整数。这就是为什么卷积核大小通常取奇数的一个原因
    - 还有一个可能的原因是，奇数大小的卷积核，代表中心的像素点只要一个。这可能更方便计算机视觉进行处理

- 卷积步长（strided convolutions）

  - 对于6x6的输入和3x3的卷积核的例子，每次卷积核移动的步长都是1。但是卷积步长可以为其他数字吗？
  - 当然可以，但是改变卷积步长同样会影响到输出的图片大小
  - 假设卷积步长stride为s，则输出的图片大小为$\lfloor\frac{n+2p-f}s+1\rfloor x \lfloor\frac{n+2p-f}s+1\rfloor$
  - 同时如果要做Same Convolutions，则$\lfloor\frac{n+2p-f}s+1\rfloor=n$

- 三维卷积（RGB images）

  - 在上面的讨论中，只考虑了灰度图像。而生活中，彩色的图像会更普遍。
  - 彩色的图片拥有三个色彩通道，即RGB Channel。这使一个RGB彩色图片的维度变为了nxmx3
    - 通常令输入的图片是一个（width，height，depth(channel)）的三维张量
  - 那么如何将CNN应用到彩色的图片上？
    - 首先将卷积核也扩展为3维的，即每个色彩通道都对应一个不同的卷积核。
    - 然后类似切多层蛋糕的方式，将图像的每个通道和通道对应的卷积核进行卷积
    - 最后将卷积得到的3个输出线性相加，即得到了RGB图像的卷积输出
  - 图例如下
    - ![RGB卷积](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/RGB%E5%8D%B7%E7%A7%AF.png)
    - ![RGB卷积2](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/RGB%E5%8D%B7%E7%A7%AF2.gif)

- 多个卷积核（Multiple filters）

  - 上面的边缘检测例子中，使用了一个卷积核获取图片的垂直边缘。如果想要获取水平的边缘，就又要使用水平的卷积核再次对图像进行卷积处理
  - 如果想同时获取图像的水平和垂直边缘，可以将两个卷积核得到的输出图像叠加起来
  - 更进一步，如果将各个角度的卷积核输出的图像组合为一个深度为n的图像，那么可以更好的对图像提取出边缘特征
  - ![多个卷积核](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%A4%9A%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8.png)


### 简单的卷积神经网络

- 一层卷积神经网络
  - 符号规定
    - 用$f^{(l)}$表示第$l$和$l-1$层之间卷积核的大小（filter size）
    - 用$p^{(l)}$表示第$l$层输入神经核的填充数量（padding）
    - 用$s^{(l)}$表示第$l$层的卷积步长（stride）
    - 用$n_h^{(l)},n_w^{(l)},n_c^{(l)}$表示第$l$层的输入或输出的图片的高（height）、宽（weight）、通道数（channel or depth）。或第$l$和$l-1$层之间卷积核的形状,同时$n_c^{(l)}$也表示卷积核的个数
    - 用$a^{(l)}$表示加上偏执项后用Relu函数激活的第$l-1$层的输出项，第$l$层的输入项
    - 设输入图片的形状和输出的形状为Input：$(n_h^{(l-1)},n_w^{(l-1)},n_c^{(l-1)})$，Output:$(n_h^{(l)},n_w^{(l)},n_c^{(l)})$
      - 则输入和输出两层的图片形状关系有：$n_{h\ or\ w}^{(l)}=\lfloor\frac{n_{h\ or\ w}^{(l-1)}+2p^{(l)}-f^{(l)}}{s^{(l)}}+1\rfloor$
      - 中间的每个卷积核的形状为：$(f^{(l)},f^{(l)},n_c^{(l-1)})$,且一共有$n_c^{(l)}$个卷积核
      - 因此需要确定的权重的个数为$f^{(l)}xf^{(l)}xn_c^{(l-1)}xn_c^{(l)}$
      - 需要为每个卷积核设定一个偏执项（bias），因此偏执项的个数=卷积核的个数$n_c^{(l)}$
  - 一个例子
    - ![一层CNN的例子](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%80%E5%B1%82CNN%E7%9A%84%E4%BE%8B%E5%AD%90.png)
    - 上图中，输入图片的形状为6x6x3,用两个3x3x3的卷积核进行卷积
      - 输出两个4x4的矩阵
      - 对两个矩阵，先为矩阵中的每个元素加上相同的偏执项b（这在python中可以用广播实现），再使用非线性的激活函数Relu进行激活，得到两个激活后的4x4矩阵
      - 将两个矩阵堆叠起来，作为输出
      - 以上操作，即完成了一层的卷积神经网络
    - 数学表达
      - 设输入$x = a^{(0)}$,所有卷积核的权重矩阵为$w^{(1)}$,未激活的输出为$z^{(1)}$
      - 则$z^{(1)}=w^{(1)}a^{(0)}+b$
      - 激活后$a^{(1)}=g(z^{(1)})$
  - 一些启发
    - 一层卷积神经网络的需要确定的权重与什么有关？
      - 一层CNN的需要确定的权重完全取决于卷积核的大小和数量
      - 这与前馈神经网络中权重由两层间的输入神经元和输出神经元的个数决定完全不同
      - 反映在输入的特征数量非常大时，前馈神经网络需要确定的权重会很多，而CNN不管输入的特征数量是多还是少，一旦模型确定后，需要确定的权重的数量是不会改变的
      - 因此CNN非常适合处理特征数量非常多的数据，这称为CNN的“避免过拟合”的特性
- 池化层（Pooling layer）
  - What？
    - 对卷积处理的输出矩阵进行的一种特殊操作
    - 最常见的Max Polling 就是取出对应输出矩阵区域中的最大值
    - 例子![Pooling](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/Pooling.png)
    - 如上图，相当于用一个类似f=2，s=2，p=0的卷积核窗口对输出矩阵进行了一种特殊处理。也就是取出对应区域中的最大值
    - 相当于对图像进行了下采样（SUbsamping）
  - why？
    - 直观理解
      - 在边缘检测中，输出矩阵出现比较大的数值，可能往往代表发现了某个特征或边缘。
      - 因此在CNN中会提取这个特征，也就是保留这个最大值
      - 用类似卷积核的窗口扫描整个输出矩阵，可能右上角的区域经过卷积操作出现了某个边缘或特征，保留这个最大值。而左上角没有，就算取这个区域的最大值，这个最大值也不会很大
    - 好处
      - 减少模型的规模，同时又不太影响模型的性能
      - 提高计算速度
      - 提高鲁棒性
        - 
  - 池化层的超参数
    - 类似于卷积核的参数，池化层需要确定对应的超参数形成一个类似于卷积核的窗口以便划分区域，但也仅仅而已。池化层不需要进行卷积操作，也不需要去优化其中的权重（因为没有）
    - 需要确定的超参数：池化层卷积核的大小f，以及卷积步长s，通常不需要填充（padding）p=0
    - 比较常用f=2，s=2的超参数，则池化层大小会比原输出的大小小一半
  - 池化层的类型
    - Max Polling：取每个区域的最大值，这是最常使用的池化层类型
    - Average Polling：取每个区域的平均值
- 全连接层（FC layer）
  - 其实就是之前第五章学过的前馈神经网络
  - why？
    - 之前用卷积层和池化层的到的输出矩阵，相当直接在原图片上提取出二维的特征空间
    - 然而为了使用“分类器”的方法，需要将特征空间展开为线性的（一维）
    - 因此在一维的特征空间中进行训练，当然要使用前馈神经网络即全连接层

### 整合起来——一套完整的CNN

- 手写数字识别的例子
- ![一个完整的CNN](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84CNN.png)
- 流程
  - 输入一个32x32x3的手写数字图片
  - 用f=5，s=1，p=0的6个卷积核卷积，输出28x28x6的矩阵。然后经过f=2，s=2的池化层，输出14x14x6的矩阵。这样的一个卷积层+池化层视为神经网络中的一层
  - 继续用f=5，s=1的16个卷积核卷积并使用f=2，s=2的池化层，输出5x5x16的矩阵
  - 展开输出结果，为（400,1）的向量，然后进入全连接层
  - 最后通过softmax层输出（10,1）的向量，进行手写数字的判断
- 流程中的参数变化
  - ![CNN中参数变化](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/CNN%E4%B8%AD%E5%8F%82%E6%95%B0%E5%8F%98%E5%8C%96.png)
  - 在进入全连接层之前，$n_h$和$n_h$变得越来越小，而$n_c$变得越来越大
  - 整个流程中需要激活的神经元变得越来越少，但是要注意激活值如果减少的太快，会影响模型的性能
  - 整个流程中，除了输入层和池化层不需要确定参数外，其他层需要确定的参数到全连接层急剧增加
- 流程中的超参数
  - 超参数的确认，暂时建议参考别人论文中成熟的模型的超参数
- 为什么需要卷积？
  - 卷积优于全连接的地方在于参数共享和稀疏连接，这就保证了卷积层需要确定的权重远远少于全连接层
    - 参数共享：一个特征提取器（卷积核）可以适用于图片的任意部分
    - 稀疏连接：每层的输出的每个神经元都由一小部分输入神经元（fxf个）决定

### 经典网络与残差网络

- 经典网络
  - LeNet-5
  - AlexNet
  - VCG-16
- 残差网络（Residual Network/ResNet）
  - 直觉告诉我们，神经网络层数越深，则模型的性能也应该更好。但是由于**梯度消失和梯度爆炸**的问题。传统的神经网络在深度增加到一定程度的时候，模型性能反而会下降。但是残差网络的提出可以帮我们解决这个问题
  - 实现方法
    - 通过跳层连接，可以将某层的激活值传给更深层次的网络进行激活。
  - 残差块（Residual Block）
    - ![残差块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E6%AE%8B%E5%B7%AE%E5%9D%97.png)
  - 如上图所示，展示了残差网络中的一个基本单元——残差块
    - 按照经典网络中的激活方法，$a^{(l)},a^{(l+1)},a^{(l+2)}$三层的激活由各自的权重、偏执项以及非线性激活函数Relu完成
    - 而残差网络中，运用**跳层连接**。让$a^{(l)}$参与到了$a^{(l+2)}$的激活
  - 残差网络
    - ![残差网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C.png)
    - 多个残差块的加入，使经典网络变成了残差网络
    - 残差网络很好的解决了随着神经网络的层次增加到一定的层次后，性能变差的问题。帮助人们构建更深层的网络
  - 为什么残差网络能够在层数增加的同时保证神经网络的性能？
    - ![为什么残差网络有用](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E6%9C%89%E7%94%A8.png)
    - 假设已经用一个深层的网络得到了一个激活项$a^{(l)}$,在这个网络的基础上再增加两层，并进行跳层连接构建一个残差块的到新的激活项$a^{(l+2)}$
    - 则$a^{(l+2)}=g(w^{(l+2)}a^{(l+1)}+b^{(l+1)}+a^{(l)})$
      - 因为正则化和权重退化的等原因，更深层次的权重和偏执项会渐进于0
      - 因此超级深层的线性激活项也会趋近于0，因此用Relu激活时，激活项趋近0。这表示该层已经很难再训练模型了
      - 然而残差块的加入，让即使深层的激活项已经趋近于0，也可得到$a^{(l)}=a^{(l+2)}$的结果
      - 也就是说残差网络让深层的层次最差也是学习到了使两层相等的方法，这对神经网络的性能有一个保底作用。即残差块的加入让层次增加的同时，神经网络的性能不会下降。当然如果层次增加神经网络的性能也增加了，这更是乐于见到的
      - 换言之：**残差块最差也能学会恒等函数**，这为神经网络的性能兜了底
  - 注意事项
    - 残差网络因为跳层连接的存在，因此最好要保证卷积操作时Same convolution。才能保证每层激活项维度相同
    - 当然如果不保证Same convolution，就需要为跳层连接中后面层次的激活项设定一个权重，使两层的激活项维度相同

### 1x1卷积/network in network（了解）

- 对信道或深度为1的数据用1x1卷积核卷积，视乎只能使数据乘上一个常数
- 对信道>=2的矩阵使用1x1卷积核
  - 池化层能够缩减原矩阵的高和宽，而1x1卷积核能够减少或增加原矩阵的信道。比如用20个1x1的卷积核卷积20x20x200的矩阵，可以得到20x20x20的输出
  - 如果将矩阵中信道一样的所有元素视为一个个神经元，对其做1x1卷积操作。则相当于对这些神经元进行了全连接操作。输入原始矩阵信道个数的神经元，输出1x1卷积核个数的神经元
- 有什么作用？
  - 缩减信道，构建瓶颈层（bottomneck），缩小计算规模
  - 应用于其他卷积神经网络的框架中，比如下面要提到的inception网络

### Inception网络（了解）

- What？
  - 使用inception模块来帮助人们确定在CNN框架构建中，需要多大的过滤器？需不需要池化层等？
- inception模块
  - ![inception模块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E6%A8%A1%E5%9D%97.png)
  - 思想：
    - 如上图，在不确定使用1x1、3x3还是5x5的卷积核和池化层时
    - 可以对原始矩阵做上述的一系列操作。但是要保证相同卷积
    - 然后将所有不确定操作得到的输出的信道连接起来，就形成了一个inception模块
    - 让神经网络自己学习需要多大的卷积核以及什么样的池化层
  - 计算量问题
    - 虽然让神经网络自己学习自己的构造，听起来很美好。但是在网络中额外构建这样的inception模块会消耗非常多的计算资源
    - 如何减少计算量？
      - 以上图中5x5卷积操作为例，32个5x5卷积核卷积（28,28,192）输出（28,28,32）的矩阵需要进行5x5x192x28x28x32大约1.2亿次乘法运算
      - 而先运用16个**1x1卷积核**将原矩阵的信道数缩减，在使用32个5x5的卷积核输出（28,28,32）的矩阵。需要进行28x28x16x192+28x28x32x5x5x16大约240万次计算，计算量相对直接计算减少到了10%
      - ![在inception中用1x1卷积](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%9C%A8inception%E4%B8%AD%E7%94%A81x1%E5%8D%B7%E7%A7%AF.png)
- inception网络
  - 使用1x1卷积核减少计算量后的inception模块
    - ![inception网络模块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97.png)
    - 如上图所示，对卷积操作先进行1x1卷积在进行nxn的卷积，而对池化层（相同卷积）操作后再进行1x1卷积都可以缩减信道。达到减少计算量的目的
  - 多个上述inception模块，构成inception网络
    - ![inception网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E7%BD%91%E7%BB%9C.png)

### 迁移学习（transfer learning）（了解）

- what？
  - 举个例子，如果已经训练好一个可以分1000个类别的分类器。而现在又需要训练一个可以分100个类别的分类器，该怎么做？
  - 重新训练肯定是最低效的，一个比较合理的方法是，改变之前1000个类别分类器的模型的softmax层，使其输出为100个类别。同时不改变其他层次的权重和参数
  - 这就是迁移学习的核心：仅仅改变已经训练好的网络的一部分，以便适应新的、类似的模型
- 使用迁移学习
  - 当下载别人已经训练好的模型或自己的模型后，根据现在需要新训练的模型而加入的新样本的多少。来确定需要冻结多少层次（即不改变这些层次），以及重新训练多少层次
  - 一般而言，冻结是从前面的层次开始的。因为后面层次训练的参数和权重会更贴近于训练的样本的特点，也就是后面层次更加抽象更加符合训练样本的个性。
  - 以及根据输出要求的改变而重新训练对应的softmax层
  - 甚至不改变冻结的层次，而剩下的层次按照新的要求直接重塑其框架，再训练

### 数据扩充（data augmentation）（了解）

- 深度学习中，特别在计算机视觉中，训练集的数量十分缺少。因此需要使用数据扩充的方法
- 数据扩充的方法
  - 最简单的：对原始图像进行镜像，随机裁剪，旋转，适度扭曲等操作
  - 色彩转换（color shifting）：对RGB三个通道加上不同的失真值
    - ![色彩转换png](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E8%89%B2%E5%BD%A9%E8%BD%AC%E6%8D%A2png.png)
    - PCA色彩扩充
  - 结合使用
    - 比如在原始数据图像中取出一部分，先进行镜像，随机裁剪等操作，在进行色彩转换
    - 然后将得到的一批数据集代入神经网络模型中进行训练
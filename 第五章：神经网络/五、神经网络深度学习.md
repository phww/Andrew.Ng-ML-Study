

## 五、神经网络/深度学习

[TOC]



### 1.为什么要使用神经网络

#### 1.1特征空间再次膨胀

- 之前我们学习的**线性回归**需要的特征数量和问题原本的**特征数量几乎是一致的**
- **逻辑回归**为了更好的拟合决策边界，需要对原始特征进行**映射**。特征空间以指数级开始增长，但是为了防止过拟合，**也不会将特征空间弄得太大**
- 但是当特征空间达到十万、百万级别时。传统的机器学习算法的效率开始下降，人们需要寻找一种新的方法。而**神经网络**就能够很好的解决拥有**极大特征空间**的问题

#### 1.2问题不在是线性的（非线性问题）

- 线性回归解决的是**连续**的线性问题

- 逻辑回归解决的**离散**的线性问题（最终表现出来的是拟合一个决策边界，所以是线性的）

- 而神经网络能够很好的解决非线性问题

  

### 2.从人脑到神经网络（闲扯）

#### 2.1大脑就是最强的学习器

- 目前让计算机模拟大脑工作的原理，几乎做不到
- 但是人工智能学家能够从大脑的一些特性中发现一些奇特的现象，并尝试用计算机来模拟这种现象

#### 2.2神经重接

- 生物学中人的大脑中的各个部分，能分别负责控制人一些生理现象，比如听觉皮层负责处理声音信息；视力皮层负责处理视觉信息....
- 但是生物学家发现，将小动物的视力神经切断然后嫁接到听力皮层上。听力皮层会自动学会将输入的视觉信息转换为大脑中的图像。这就是神经重接试验。
- 大脑的各个部位虽然一般只负责自己区域的工作，但是各部位也能学习将各种输入信息转换为对应的输出信息
- 那么是否有一种算法也可以输入各种信息，然后通过中间的”大脑皮层“，自动学习如何处理这些信息并输出——这就是神经网络在生物学上的启发

#### 2.3生物的神经网络到神经网络算法

- 在生物学中，大脑皮层由多个神经网络组成，而多个神经元又组成神经网络<img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/jbXt6W2R9NYClom.jpg" alt="生物神经元" style="zoom:100%;" />

- 在生物的神经元中，多个树突接受电信号，传导给细胞核处理；处理后的信息沿着轴突和神经纤维传给轴突末梢；轴突末梢又将电信号传给相邻的神经元

  - 因此在神经网络算法中的对应关系为：树突——输入、细胞核——激活函数、轴突、神经纤维、轴突末梢——输出

- 可以想象生物学中多个神经元相互连接共同构建出了一套神经网络系统，而计算机模拟的神经元同样相互连接共同构建出了一个神经网络算法

  

### 3. 神经网络的实现原理

#### 3.1神经网络的构成元素

##### 神经元

- $z = \Theta{x} + \theta_0=x_1\theta_1+x_2\theta_2+...+x_k\theta_k+1*\theta_0$,称为一个神经元

- 其中$x$为单个输入样本的**特征**，$\Theta$为**参数或权重**，代表每个输入特征对神经元影响的大小

- $\theta_0$称为**偏执项**，表示周围神经元的影响和外部刺激

  ![一个神经元](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%80%E4%B8%AA%E7%A5%9E%E7%BB%8F%E5%85%83.png)

##### 非线性的激活函数

- $g(z)\to{a}$
- 神经元需要被激活才能使用
- 常用的激活函数是Sigmiod函数，即$g(z)=\frac1{1+e^{-z}}$(202105回顾:吴恩达ML视频太过遥远。目前Sigmiod很少使用了，多使用Relu激活函数)
- Relu激活函数：$g(z)=max(0,z)$

##### 神经元的连接

- 激活的神经元又作为新的神经元参与下一层的计算

- 不同的连接方式

  - 全连接：前向传播和后向传播

    **下面为21年五月回顾添加**

  - 稀疏连接：卷积层

  - 按时间序列连接：RNN

  - Transformer？？

##### 层次

- 输入层
- 隐含层
- 输出层

#### 3.2单层的神经网络

<img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%8D%95%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png" alt="单层神经网络" style="zoom:60%;" />

##### 流程

- 这是一个**前向传播**的算法
- 输入的特征向量只是**一个样本**的特征
- 其中$a_1^{(2)}$代表第二层的第一个神经元；$\Theta^{(1)}$代表将第一层映射到第2层的参数
- 可以看见

  - 第一层输入的3个特征+偏执项，作为4个神经元——输入层
  - 4个神经元在3组参数和激活函数的作用下，连接到了3个新的神经元——隐含层
    - 实际上，如果一层输入的神经元数量为n（不算偏执项）。则对应层次间，需要构造的参数矩阵的维度为(n+1)xm。即n+1个神经元（加上偏执项）作为输入，m个神经元（不加偏执项）作为输出
  - 3个新的神经元+偏执项，作为4个神经元。在1组参数和激活函数的作用下生成了一个输出——输出层
  - 最终对输出进行一些处理（softmax）或不处理得到假设函数$h_\theta(x)$——softmax层

##### 数学表达

- 上述过程的数学表达

  $a_{1}^{(2)}=g(\Theta _{10}^{(1)}{{x}_{0}}+\Theta _{11}^{(1)}{{x}_{1}}+\Theta _{12}^{(1)}{{x}_{2}}+\Theta _{13}^{(1)}{{x}_{3}})$
  $a_{2}^{(2)}=g(\Theta _{20}^{(1)}{{x}_{0}}+\Theta _{21}^{(1)}{{x}_{1}}+\Theta _{22}^{(1)}{{x}_{2}}+\Theta _{23}^{(1)}{{x}_{3}})$
  $a_{3}^{(2)}=g(\Theta _{30}^{(1)}{{x}_{0}}+\Theta _{31}^{(1)}{{x}_{1}}+\Theta _{32}^{(1)}{{x}_{2}}+\Theta _{33}^{(1)}{{x}_{3}})$
  ${{h}_{\Theta }}(x)=g(\Theta _{10}^{(2)}a_{0}^{(2)}+\Theta _{11}^{(2)}a_{1}^{(2)}+\Theta _{12}^{(2)}a_{2}^{(2)}+\Theta _{13}^{(2)}a_{3}^{(2)})$

- 矩阵表示

  将特征向量，两层间的参数，中间的神经元矩阵化
  $$
  x =\begin{pmatrix}
  x_0=1\\x_1\\x_2\\x_3\\\vdots\\x_n\\
  \end{pmatrix}
  
  \Theta^{(l)} =\begin{pmatrix}
  \theta_{10}^{(1)}\cdots\theta_{n0}^{(n)}\\
  \theta_{11}^{(1)}\ddots\theta_{n1}^{(n)}\\
  \theta_{12}^{(1)}\ddots\theta_{n2}^{(n)}\\
  \theta_{13}^{(1)}\ddots\theta_{n3}^{(n)}\\\vdots\\
  \theta_{1n+1}^{(1)}\cdots\theta_{nn+1}^{(n)}\\
  \end{pmatrix}
  a^{(l)} =\begin{pmatrix}
  a_0=1\\
  a_1\\
  a_2\\
  a_3\\
  \vdots\\
  a_{n+1}
  \end{pmatrix}
  $$
  则可简化为:

  $x.T*\Theta=a$

  $h_\Theta(x)=g(a)$

#### 3.3多层的神经网络

![多层神经网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%A4%9A%E5%B1%82%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.png)

##### 流程

- 上图是一个两层的神经网络

  - 依然还是对**一个样本**的特征进行操作
  - 可以看见这是一个3-3-2-1的神经网络，这被称为神经网络的**框架**，安照实际需求确定

- 实际的数学运算不过是再迭代几次

  - $x.T*\Theta_1=a_1\to a_1.T*\Theta_2 = a_2$

    $h_\Theta(x)=g(a_2)$

##### 复杂的神经网络

- 在多层的神经网络的基础上，将每个样本都输入该神经网络，进行训练
- 最后经过多次训练后，一个**参数稳定**的神经网络就训练好了。一个极其复杂的人造神经网络诞生了！！



####  3.4神经网络的一个例子

##### 用神经网络实现异或非门(同或门、XNOR)的计算

- 即输入的两个二值数据，相同为1，不同为0

- 因为$x_1$XNOR$x_2$ = ($x_1$AND$x_2$)OR((NOT$x_1$)AND(NOT$x_2$))
- 可以按功能实现三种功能的简单神经网络，第一种实现AND；第二种实现(NOT$x_1$)AND(NOT$x_2$)；第三个实现OR

##### 建立模型

- 输入值为两个二值数据$x_1$和$x_2$，两者取0或1

- 训练集为

  | x1   | x2   | y    |
  | ---- | ---- | ---- |
  | 0    | 0    | 1    |
  | 0    | 1    | 0    |
  | 1    | 0    | 0    |
  | 1    | 1    | 1    |

##### 思路

<img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/NN%E4%BE%8B%E5%AD%90.jpg" alt="NN例子" style="zoom:50%;" />

- AND网络

  <img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/NN%E4%BE%8B%E5%AD%90AND.png" alt="NN例子AND" style="zoom:100%;" />

- OR网络

  <img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/NN%E4%BE%8B%E5%AD%90OR.png" alt="NN例子OR" style="zoom:100%;" />

- NOT网络

  ![NN例子NOT](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/NN%E4%BE%8B%E5%AD%90NOT.png)

- 整个计算XNOR的网络

  ![NN例子整个网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/NN%E4%BE%8B%E5%AD%90%E6%95%B4%E4%B8%AA%E7%BD%91%E7%BB%9C.png)

##### 思考

- 上面的一系列过程表达了神经网络的一个核心的思想——**层层递进、由易到难**。在实际应用中。往往一个神经网络的一层解决一个小问题，然后在交给下一层解决更加困难的问题。比如在人脸识别中

- NN干了什么？以人脸识别为例

  1. 首先人们输入了一张彩色人脸图片,像素为1000X1000
  2. 对于计算机相当于输入了3X1000X1000个数字（这些数字代表每个像素点的颜色与位置）
  3. 神经网络开始工作。比如前期的层次会识别出一些图像的轮廓，中期的层次会识别出每个轮廓是什么器官（眼耳口鼻？），后期的层次会识别出每个器官的特点、分析脸型
  4. 输入大量的带有标签的彩色人脸图片，重复上述过程。最终该NN模型能学会怎样识别人脸

- 参数、权重？

  - 上述例子的参数是怎么确定的？

    - 反向传播算法+优化算法

  - 偏执项一定为一吗？

    - 视情况而定，偏执项也可以通过反向传播+优化的方法进行调整

      

### 4. 神经网络的实现——准备工作

#### 4.1<span id="jump">数据集的表示</span>

对于监督学习，如果将训练集看作表格。则训练集表格的**行索引为各个样本**，而训练集的**列索引为特征+标签**

##### 用$(x^{(i)},y^{(i)})$表示训练集中的一个样本

- 用$X$表示特征的集合（其中的特征都是列向量），$x^{(i)}$表示样本i的特征向量，$\Theta^{(l)}$表示两层间的参数矩阵，$\theta^{(n)}_{nm}$表示第n个输出神经元对应的参数向量中的第m个参数
  $$
  X =\begin{pmatrix}
  x_0\ x_1\ x_2\ x_3\ \cdots\ x_n\\
  \end{pmatrix}
  
  x^{(i)} =\begin{pmatrix}
  x_0^{(i)}=1\\x_1^{(i)}\\x_2^{(i)}\\x_3^{(i)}\\\vdots\\x_n^{(i)}\\
  \end{pmatrix}
  
  \Theta^{(l)} =\begin{pmatrix}
  \theta_{10}^{(1)}\cdots\theta_{n0}^{(n)}\\
  \theta_{11}^{(1)}\ddots\theta_{n1}^{(n)}\\
  \theta_{12}^{(1)}\ddots\theta_{n2}^{(n)}\\
  \theta_{13}^{(1)}\ddots\theta_{n3}^{(n)}\\\vdots\\
  \theta_{1m}^{(1)}\cdots\theta_{nm}^{(n)}\\
  \end{pmatrix}
  $$

- 用$y^{(i)}$表示标签，对于分类问题标签为(二分类和多分类)

$$
y^{(i)} =
  \begin{cases}
  0, & \text{neg}  \\
  1, & \text{pos}
  \end{cases}

  y^{(i)} =
  \begin{pmatrix}
  1\\0\\0\\0\\\vdots\\0\\
  \end{pmatrix}
  or
  \begin{pmatrix}
  0\\1\\0\\0\\\vdots\\0\\
  \end{pmatrix}
  or
  \begin{pmatrix}
  0\\0\\1\\0\\\vdots\\0\\
  \end{pmatrix}
  or
  \begin{pmatrix}
  0\\0\\0\\1\\\vdots\\0\\
  \end{pmatrix}
  or
  ...\begin{pmatrix}
  0\\0\\0\\0\\\vdots\\1\\
  \end{pmatrix}
$$

##### 输出层的表示

- 用$h_\Theta(x)$表示输出层输出的值，对于多分类问题这可能是一个n维向量;则$(h_\Theta(x))_i$表示输出层输出的第i个值。则$(h_\Theta(x^{(i)}))_i$表示第i个样本在输出层的第i个输出值

##### 神经网络的一些表示

- 用$L$表示神经网络的总层次，$s_l$表示不带偏执项的第$l$层的神经元个数
  - 则$s_L$表示输出层的输出神经元（单元）的个数
- 用$K$表示输出层的输出单元的个数，对于二分类K=1。多分类$K=s_L$



### 5.神经网络的实现——代价函数

#### 5.1数学表示

交叉熵损失+L2正则化

- $J(\Theta) = -\frac{1}{m} [ \sum\limits_{i=1}^{m} \sum\limits_{k=1}^{k}y_k^{(i)}log(h_{\Theta}(x^{(i)}))_k+(1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)}))_k)+ \frac{\lambda}{2m} \sum\limits_{l=1}^{L-1} \sum\limits_{i=1}^{s_l} \sum\limits_{j=1}^{s_{l+1}} \left( \Theta_{ji}^{(l)} \right)^2$

#### 5.2神经网络的代价函数 VS 逻辑回归的正则化代价函数

- $J_\Theta=\frac1{m}\sum_{i=1}^mCost(h_\theta(X^{(i)}),y^{(i)})=-\frac1m[\sum_{i=1}^my^{(i)}log(h_\theta(X^{(i)}))+(1-y^{(i)})log(1-h_\theta(X^{(i)}))+\lambda \sum\limits_{j=1}^{n}{\theta _{j}^{2}}]$这是逻辑回归的代价函数，两者之间似乎有一些联系

- $\sum\limits_{k=1}^{k}$的由来
  - 多分类的情况下神经网络**输出层输出的值为K维向量**
  - 而逻辑回归只输出一个数值。所以**将这K维向量加起来类比逻辑回归的单一数值**
  - 如果不加正则化项，默认输出层的所有输出值已经相加，神经网络的代价函数似乎和逻辑回归的代价函数是一致的(202105：因为都是使用交叉熵损失)
- 正则化项
  - 逻辑回归只有一组参数向量,正则化只用考虑一组向量
  - 而神经网络的每一层都有一组((n+1)xn)维的参数矩阵，整个神经网络有L层的((n+1)xn)维的参数矩阵（202105：前提是FC层每层的输入输出神经元个数都是一样的）
  - 因此神经网络的的参数矩阵的形状为(L,n+1,n)
  - 则神经网络正则化，需要先将每层的((n+1)*n)维参数矩阵先对行参数平方求和，再对列参数平方求和，得到每层的平方参数和（这是一个值）。最后对层参数平方求和
  - 最后达到对L\*（n+1）*n个参数进行正则化的目的
- $h(X)$还是$h(x)$?
  - 之前所学习的线性回归和逻辑回归，的优化算法准确来讲是批量梯度下降法
  - 所谓**批量**，即对所有参数一起进行梯度下降；且每次迭代，所有样本一起参加计算
  - 所以逻辑回归和线性回归的代价函数中为$h(X)$
  - 但是神经网络是将样本一个个输入到网络中，让神经网络针对每一个样本进行一次参数的微调
  - 因此神经网络的代价函数中为$h(x)$

#### 5.3python实现

```python
"""这是一个包含一个隐含层，一个输出层和输入层的神经网络"""
"""一些函数请看，神经网络的实现——一些细节处理章节"""
"""前向传播"""
def feedforward(theta, X,):
    theta1,theta2 = rolling_parameters(theta)
    a_1 = X
    z_2 = theta1 @ a_1.T
    a_2 = sigmoid(z_2)
    a_2 = np.insert(a_2, 0, np.ones(X.shape[0]), axis = 0)
    z_3 = theta2 @ a_2
    a_3 = sigmoid(z_3)
    h = a_3.T
    return a_1, z_2, a_2, z_3, h


"""代价"""
def costfunction(theta,X, y,):
    _,_,_,_,h = feedforward(theta,X,)
    pos = -np.log(h) * y
    neg = -np.log(1 - h) * (1 - y)
    return 1/len(X)*np.sum(pos + neg)


"""正则化代价"""
def costfunction_reg(theta,X, y, l = 1):
    theta1, theta2 = rolling_parameters(theta)
    reg = l/(2 * len(X)) * (np.sum(theta1[:,1:]**2) + np.sum(theta2[:,1:]**2))
    return costfunction(theta, X, y) + reg
```



### 6.神经网络的实现——反向传播算法(BackPropagation)

#### 6.1思想

##### 反向传播的目的

- **找到一组的参数**$\Theta$，令代价$J(\Theta)$最小  <u>(202105:拟合一个函数逼近器的参数</u>)

- 在神经网络的**前向传播**的过程中，每一层的神经元$a^{(l)}$都受参数$\Theta$的影响
- 影响的结果是:最终输出层的输出值,即$a^{(L)}$会因为参数$\Theta$的改变而改变
- 一组好的参数$\Theta$会令代价$J(\Theta)$最小，这反映到神经网络中就是**输出层的输出值$a^{(L)}$与标签$y^{(i)}$的差距足够小**

##### 搞清楚什么是代价（误差）

- 假设神经网络输出层输出的值为$a^{(L)}$，误差为$\delta^{(L)}$,则输出层的输出值与标签的误差为：$\delta^{(L)}=a^{(L)}-y^{(i)}$
- 而输出层的误差源自上一层的输入值$a^{(L-1)}$在参数$\Theta^{(L)}$作用下的结果，上一层来自上上层的影响...

##### 解决误差的方法

- 误差的根源就是参数$\Theta$不够精确，神经网络的BP算法利用每个样本的输出值与标签的误差反向传播找到根源，从而用一个样本更新一次$\Theta$，最终得到较为精准的参数。
- 设定一个初始的参数$\Theta$,将一个样本代入神经网络中，得到一个输出值。输出值和标签肯定有误差，要想最小化误差,可以沿着**梯度下降**的方向更新参数$\Theta$。为了了解决误差，需要找到神经网络中每个参数对应的梯度。而输出层的梯度可以轻松获得，而其他层次的梯度需要依靠梯度流的反向传播获得。

**<u>202105补充:梯度是怎么反向传播的</u>**

- 反向传播算法是基于梯度下降的方法进行优化

  - 根据前向传播可以绘制出整个流程的**计算图**
  - 计算图的每个计算节点都有自己的**局部梯度**：比如该节点的算式为：$f(x)=x^2$且由前向传播知道x=5,则该节点的局部梯度为$2*x=2*5=10$
  - 后向传播时一个计算节点的梯度=自己的局部梯度 * 上一层传过来的梯度流（上一层：方向为从后向前）

- 例子：

  <img src="https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210523165806787.png" alt="image-20210523165806787" style="zoom:150%;" />

  

#### 6.2算法实现(基于吴恩达的视频)

##### 从前向传播到反向传播

- 令第$l$层第i个没有激活的神经元为$z^{(l)}_i$

- 则前向传播两层之间的关系为：$z^{(l+1)}=(\Theta^{(l)})^T a^{(l)}$

  ​													$a^{(l+1)}=g(z^{(l+1)})$

- 反向传播可视为前向传播的逆运算：$\delta^{(l+1)}=a^{(l+1)}-y$

  ​															$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}\bullet g^{`}(z^{(l)})$

  因为sigmoid函数的导数有个性质，即：$f^{`}(x)=f(x)(1-f(x))$,所以逆运算可以转化为：  

  ​                                          						$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}\bullet ((a^{(l)})(1-a^{(l)}))$																							

##### 用梯度下降算法来优化代价函数

- 回忆梯度下降算法，$\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\Theta)=$$\theta_j-\alpha\frac1m\sum_{i=1}^m(h_\theta(X)^{(i)}-y^{(i)})x_j^{(i)}$
  - $h_\theta(X)^{(i)}-y^{(i)})$表示的就是**误差**，在神经网络中就是$\delta^{(l)}$，不过神经网络的误差是k维向量
  - $x_j^{(i)}$表示第i个样本的第j个分量，在神经网络的每一层中，同样有多个分量，即$a_i^{(l)}$。表示影响误差的输入值
  - 则神经网络的梯度下降算法第$l$层的第i个误差与上一层的第j个输入神经元对应的偏导项可以表示为$\Delta_{ij}^{(l)}=\alpha\frac1m(\delta^{(l+1)}_ia_j^{(l)})$
  - 将每个误差和上一层的每个输入值对应的偏导数用矩阵表示，即则每一层的偏导项对应的向量表达为：$\Delta^{(l)}=\delta^{(l+1)}(a^{(l)})^T$，这是个ixj维的偏导数矩阵。与之前的梯度下降对比，因为后向传播每次只考虑一个样本，因此没有$\sum_{i=1}^m$操作
- 添加正则化项：
  - 因为偏执项不用正则化，当j=0时：$D_{ij}^{(l)}=\frac1m\Delta_{ij}^{(l)}$
  - 其他输入项，当$j\ne0$时：$D_{ij}^{(l)}=\frac1m\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$
  - 即梯度下降中的偏导项（梯度）为：$\alpha\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)=\alpha D_{ij}^{(l)}$
- 用梯度下降对每层的参数进行优化
  - $\Theta_{ij}^{(l)}:=\Theta_{ij}^{(l)}-\alpha\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)$

##### BP算法的过程

![BP过程吴恩达](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/BP%E8%BF%87%E7%A8%8B%E5%90%B4%E6%81%A9%E8%BE%BE.png)

##### python实现

```python
"""这是一个包含一个隐含层，一个输出层和输入层的神经网络"""
"""一些函数请看，神经网络的实现——一些细节处理章节"""

"""计算每个权重的梯度"""
def bp_gradient(theta,X, y):
    theta1,theta2 = rolling_parameters(theta)
    Delta_1 = np.zeros(theta1.shape)
    Delta_2 = np.zeros(theta2.shape)
    a_1, z_2, a_2, z_3, h = feedforward(theta,X)
    for i in range(X.shape[0]):
        a_1i = a_1[i,:]
        z_2i = z_2[:,i]
        a_2i = a_2[:,i]
        z_3i = z_3[:,i]
        h_i = h[i,:]
        y_i = y[i,:]
        delta_3i = h_i - y_i
        z_2i = np.hstack([1,z_2[:,i]])
        delta_2i = theta2.T @ delta_3i * sigmoid_gradient(z_2i)
        Delta_2 = Delta_2 + delta_3i[:,np.newaxis] @ a_2i[:,np.newaxis].T
        Delta_1 = Delta_1 + delta_2i[1:,np.newaxis] @ a_1i[:,np.newaxis].T
    return unrolling_paramteters(Delta_1/len(X), Delta_2/len(X))



"""正则化梯度"""
def bp_gradient_reg(theta, X, y, l=1):
    Delta1, Delta2 = rolling_parameters(bp_gradient(theta,X, y,))
    theta1, theta2 = rolling_parameters(theta)
    theta1[:, 0] = 0
    Delta1_reg = (l / len(X)) * theta1
    Delta1 = Delta1 + Delta1_reg
    t2[:, 0] = 0
    Delta2_reg = (l / len(X)) * theta2
    delta2 = Delta2 + Delta2_reg
    return unrolling_paramteters(Delta1, Delta2)
```



#### 6.3算法数学推导（基于周志华的机器学习）

##### 输出层的链式法则

- 反向传播的**链式法则**，通俗上将就是找到每一层的参数影响误差的“路线”
- 比如，输出层的参数的一个分量$\Theta^{(L)}_{ij}$,只能影响一个输出层的输入分量$z^{(L)}_i$，然后通过激活函数影响输出分量$$a^{(L)}_i$$,进而这个分量能影响误差分量$\delta^{(L)}_i$
- 用数学语言表示就是，$\delta^{(L)}_i$是$a^{(L)}_i$的函数，$a^{(L)}_i$是$z^{(L)}_i$的函数，$z^{(L)}_i$是$\Theta^{(L)}_{ij}$的函数
- 所以当要用梯度下降法优化时，参数$\Theta^{(L)}_{ij}$对应误差函数的梯度为$\frac{\partial}{\partial\Theta_{ij}^{(L)}}\delta^{(L)}_j=\frac{\partial{z_j^{(L)}}}{\partial\Theta_{ij}^{(L)}} \frac{\partial{a_j^{(L)}}}{\partial z^{(L)}_j} \frac{\partial{\delta_j^{(L)}}}{\partial a_j^{(L)}}$
- 用均方误差（$\delta^{(L)}_j = \frac12\sum_{j=1}^l(a_j^{(L)}-y_j)$）表示输出层的误差时，根据前向传播的各函数式，上式可变为：$\frac{\partial}{\partial\Theta_{ij}^{(L)}}\delta^{(L)}_j=(a^{(L)}_j-y_j)a^{(L)}_j(1-a^{(L)}_j)a^{(L-1)}_j$

##### 隐含层的链式法则

- 输出层的链式法则是基于**输出和标签**的误差开始反向传播的。但是**隐含层并没有对应的标签**，因此不能直接构建隐含层的误差函数。但是可以**用输出层的误差来间接的表示隐含层的误差**
- 比如，隐含层参数的一个分量$\Theta^{(l)}_{ij}$,只能影响下一层的输入分量$z^{(l)}_i$，然后通过激活函数影响输出分量$a^{(l)}_i$。但是这个分量能够影响再下一层的所有输入$z^{(l+1)}$，进而通过激活函数影响输出$a^{(l+1)}$，最后正向传播到输出层，影响所有的误差$\delta^{(L)}$
- 总结一下就是：隐含层的每个参数先影响下一隐含层的一个输入值、输出值。然后影响了输出层的所有输入，输出以及误差
- 所以当要用梯度下降法优化时，隐含层参数$\Theta^{(l)}_{ij}$对应误差函数的梯度为$\frac{\partial}{\partial\Theta_{ij}^{(l)}}\delta^{(L)}=\frac{\partial{z_j^{(l)}}}{\partial\Theta_{ij}^{(l)}} \frac{\partial{a_j^{(l)}}}{z_j^{(l)}} \sum_{j=1}^{k}\frac{\partial{z_j^{(L)}}}{\partial a_j^{(l)}} \frac{\partial{a_j^{(L)}}}{\partial z^{(L)}_j} \frac{\partial{\delta_j^{(L)}}}{\partial a_j^{(L)}}$
- 用均方误差表示输出层的误差时，根据前向传播的各函数式，上式可变为:$\frac{\partial}{\partial\Theta_{ij}^{(l)}}\delta^{(L)}_j=a^{(l-1)}_j a^{(l)}_j(1-a^{(l)}_j) \sum_{j=1}^k(a^{(L)}_j-y_j)a^{(L)}_j\Theta_{\_j}^{(L)}$

- 再次简化
  - 用$g_j= \frac{\partial{a_j^{(L)}}}{\partial z^{(L)}_j} \frac{\partial{\delta_j^{(L)}}}{\partial a_j^{(L)}} = (a^{(L)}_j-y_j)a^{(L)}_j(1-a^{(L)}_j)$
  - 用$e_h= \frac{\partial{a_j^{(l)}}}{z_j^{(l)}} \sum_{j=1}^{k}\frac{\partial{z_j^{(L)}}}{\partial a_j^{(l)}} \frac{\partial{a_j^{(L)}}}{\partial z^{(L)}_j} \frac{\partial{\delta_j^{(L)}}}{\partial a_j^{(L)}} = a^{(l)}_j(1-a^{(l)}_j) \sum_{j=1}^k(a^{(L)}_j-y_j)a^{(L)}_j\Theta_{\_j}^{(L)} = a^{(l)}_j(1-a^{(l)}_j) \sum_{j=1}^kg_j\Theta_{ij}^{(L)}$
  - 输出层的参数$\Theta^{(L)}_{ij}$，对应误差函数的梯度表示为：$\Delta_{\Theta_{ij}^{(L)}} = \eta g_ja_j^{(L-1)}$,$\eta$表示学习率
  - 隐含层的参数$\Theta^{(l)}_{ij}$，对应误差函数的梯度表示为：$\Delta_{\Theta_{ij}^{(l)}} = \eta e_ha_j^{(l-1)}$

##### 偏执项

- 推导过程略：
- 输出层的偏执项的梯度为：$\Delta_{b^{(L)}}=-\eta g_j$
- 隐含层的偏执项的梯度为：$\Delta_{b^{(l)}}=-\eta e_h$

##### BP算法的过程

![BP过程周志华](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/BP%E8%BF%87%E7%A8%8B%E5%91%A8%E5%BF%97%E5%8D%8E.jpg)

- BP的目标函数

  - BP的目标是**最小化训练集的积累误差**：$\Delta=\frac1K\sum_{j=1}^K\delta^{(L)}_j$

- 正则项

  - 在积累误差函数中增加描述网络复杂度的部分——连接权（参数）和阈值（偏执项）的平方$w_i^2$
  - $\Delta=\frac\lambda K\sum_{j=1}^K\delta^{(L)}_j+(1-\lambda)\sum_iw_i^2 $，其中$\lambda\in(0,1)$

  

### 7.神经网络的实现——一些细节处理

#### 7.1参数展开(Unrolling Parameters）(202105：这不重要，numpy本来就支持多维矩阵的运算)

- why

  - 神经网络的参数和计算的梯度都是多维矩阵，每一层的参数和每个参数对应的梯度是一个nx（n+1)的矩阵。吴恩达的代码中**需要将二维矩阵转换为一维向量**
  - 还有个原因是：scipy和sklearn中封装好的高级优化算法要求传入的梯度和初始化参数是一维向量，这也需要参数展开的帮助

- how

  - 用一个一维向量来存储每层展开的参数或梯度

  - python实现

  ```python
  """这是一个包含一个隐含层，一个输出层和输入层的神经网络"""
  """参数展开"""
  def unrolling_paramteters(a, b):
      return np.hstack([a.ravel(), b.ravel()])
  
  """参数还原"""
  def rolling_parameters(theta):#根据NN的结构还原
      theta1 = theta[:25*401].reshape((25, 401))
      theta2 = theta[25*401:].reshape((10, 26))
      return theta1,theta2
  ```

- 注意事项

  - 将参数和梯度展开只是为了方便代入已有的梯度下降算法进行计算，和反向传播算法本身没有关系
  - 用反向传播算法**计算每个参数的梯度时**，使用的还是原始**未展开的参数**
  - 只有最后代入**梯度下降**算法进行计算时，才需要将参数和梯度**展开**

#### 7.2梯度检测(Gradient check)

- why

  - 某些情况下，梯度的计算出现了问题，但程序却不会报错
  - 人们需要一种能检测梯度算法是否正确运行的方法——梯度检测

- how

  ![梯度检测](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%B5%8B.png)

  - 在参数的两侧取一个比较小的数$\epsilon$，用$(\Theta+\epsilon,J(\Theta+\epsilon))$和$(\Theta-\epsilon,J(\Theta-\epsilon))$**两点间的斜率**来近似估计参数$\Theta$的斜率（梯度）
  - 估计值和实际值相差很小时，可认为梯度算法运行无误

- 在神经网络中运用梯度检测

  - 上面的参数只是一个，而神经网络中的参数是多维矩阵形式的

  - 因此首先需要将参数展开为一维向量

  - 然后将展开的参数代入代价函数中，针对每一个参数分量进行梯度检测

  - 具体数学表达如下
    $$
    \frac{\partial J(\Theta)}{\partial\theta_1}\approx\frac{J(\theta_1+\epsilon,\theta_2,\theta_3,\cdots,\theta_n) - J(\theta_1-\epsilon,\theta_2,\theta_3,\cdots,\theta_n)}{2\epsilon}\\
    \frac{\partial J(\Theta)}{\partial\theta_2}\approx\frac{J(\theta_1,\theta_2+\epsilon,\theta_3,\cdots,\theta_n) - J(\theta_1,\theta_2-\epsilon,\theta_3,\cdots,\theta_n)}{2\epsilon}\\
    \frac{\partial J(\Theta)}{\partial\theta_3}\approx\frac{J(\theta_1,\theta_2,\theta_3+\epsilon,\cdots,\theta_n) - J(\theta_1,\theta_2,\theta_3-\epsilon,\cdots,\theta_n)}{2\epsilon}\\\vdots\\
    \frac{\partial J(\Theta)}{\partial\theta_n}\approx\frac{J(\theta_1,\theta_2,\theta_3,\cdots,\theta_n+\epsilon) - J(\theta_1,\theta_2,\theta_3,\cdots,\theta_n-\epsilon)}{2\epsilon}
    $$

- 注意事项

  - 梯度检测更多是用来事先检测，神经网络的反向传播算法中的梯度是否计算准确
    - 先跑几轮反向传播算法，将实际梯度和预测梯度比较，保证梯度算法正常
    - **一旦开始神经网络的训练，关掉梯度检测**

- python实现

```python
def gradient_check(thetaVec,gradAnalytic):
    Epsilon = 0.00001
    thetaMinus = thetaVec
    thetaPlus = thetaVec
    for i in range(len(thetaVec)):
        thetaPlus[i] = thetaPlus[i] + Epsilon
        thetaMinus[i] = thetaMinus[i] - Epsilon
        gradApprox = (costfunction(thetaPlus,X,y) -costfunction(thetaMinus,X,y))/(2 * Epsilon)
        diff = np.mean(gradApprox - gradient)
    return diff
#运行很慢！！
```

#### 7.3随机初始化

- why

  - 之前的线性回归和逻辑回归都默认初始参数为全一或全零，这是被允许的
  - 而在神经网络中如果所有参数都设定一样，那么所有的输出神经元都是相同的
  - 相当于整个神经网络只是针对一个特征进行训练，因此神经网络中的参数都需要被随机赋值。当然要保证所有参数的随机值都在一个小范围内，比如0到1

- python实现

  ```python
  #用0-1内的数随机初始化矩阵
  theta1 = np.random.rand(n,n+1) 
  theta2 = np.random.rand(n,n+1)
  #用a-b内的数，随机化初始整数矩阵
  theta1 = np.random.randint(a, b, size=(c, d))
  theta1 = np.random.randint(a, b, size=(c, d))
  #用正态分布随机初始化保证，数据的对称性
  def randinit_wights(n, epsilon = 0.12):
      return np.random.uniform(-epsilon, epsilon, n)
  ```

#### 7.4 sigmoid函数的导数

- 反向传播的过程中涉及到sigmiod函数的导数。设$f(x)$代表sigmoid函数，其导数为：$f(x)(1-f(x))$

- python实现

  ```python
  """sigmoid函数的导数"""
  def sigmoid_gradient(z):
      return sigmoid(z) * (1 - sigmoid(z))
  ```



### 8.神经网络——总结

#### 8.1组合——从头开始组建神经网络

- 确定神经网络的框架（结构）[数据集的准备工作](#jump)
  - 输入层的输入神经元的个数——特征的维度
  - 输出层的输出神经元的个数——分类的个数
  - 隐含层的神经元个数，与隐含层的层数

#### 8.2训练神经网络

##### 随机初始化权重（参数）

#####  前向传播

- 对一个样本$(x^{(i)},y^{(i)})$，先运用前向传播算法得到输出值$h_\Theta(x^{(i)})$
- 再用代价函数计算在该权重下的代价

##### 反向传播

- 最后使用反向传播算法计算每个参数的梯度（偏导数）
- 将梯度检测的近似值与反向传播得到的实际进行比较，确定反向传播算法运行无误后关闭梯度检测

##### 优化器迭代

- 使用各种优化算法(比如梯度下降)对权重进行一次优化（注意参数展开的情况）
- 对剩下的样本全部实现一次上述流程（关闭梯度检测），得到最终优化后的权重。即训练好的神经网络

##### 验证神经网络的性能

<u>202105回顾：实际训练中通常是读取**一个批次**的数据执行上面的过程。直到整个数据集全部被处理过后,代表完成了一次整体数据集的迭代</u>



### 9.编程实现

基于吴恩达ML作业：ex4_Neural Networks Learing



2020/9/29

<u>2020/05/22修订</u>

---
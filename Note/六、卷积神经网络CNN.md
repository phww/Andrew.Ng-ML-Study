

## 六、卷积神经网络CNN

[github对应md文档](https://github.com/phww/Andrew.Ng-ML-Study/tree/main/Note)

[参考视频：吴恩达深度学习第三课-卷积神经网络week1-2](https://www.bilibili.com/video/BV1F4411y7o7?p=1)

[参考博客：知乎：CNN](https://www.zhihu.com/question/52668301/answer/194998098?utm_source=hot_content_share&utm_medium=all)

[TOC]



### 1.基础知识

#### 1.1从边缘检测说起

- 如下图所示，将一个图片的灰度矩阵和一个特殊的矩阵进行一种特殊的操作——卷积。就可以得到原始图片的垂直边缘

  *202105:这个**特殊的矩阵（filter）**以前在机器学习的时代是要人工设置的也就时filter中的参数是人为构造的。但是深度学习的出现，filter中的参数会被自动学习到。虽然这种自动学习到的参数缺少可解释性。*

  ![垂直边缘检测](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%9E%82%E7%9B%B4%E8%BE%B9%E7%BC%98%E6%A3%80%E6%B5%8B.png)

- 图中第二个特殊矩阵的正式的名称为**过滤器（filter）或核（kernel）**。原始6 x 6图片和3 x 3的过滤器进行了卷积操作，从而获得了一个4 x 4的图片。可认为生成的图片的中间部分就是原始图片的垂直边缘，虽然看起来这个边缘很宽，但是这是因为原始图片只是6 x 6的大小，如果是1000 x 1000的大小，中间的分割线就看着不宽了



#### 1.2什么是卷积操作？

- 百度百科：在泛函分析中，卷积、旋积或摺积(英语：Convolution)是通过两个函数f 和g 生成第三个函数的一种数学算子，表征函数f 与g经过**翻转和平移**的重叠部分函数值乘积对重叠长度的积分。

  *202105：CNN中的卷积**没有翻转**的过程。简言之就是kernel框选住的区域，对应位置的**数值加权之和***

- 直观的gif

  ![卷积神经网络动图](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8A%A8%E5%9B%BE.gif)

  - 与前向传播对比，可以发现：第一层的输入神经元（像素点）并**不全部**参与输出神经元的计算，这是卷积神经网络和普通的神经网络的一大区别（**稀疏连接**）。因此前向传播的神经网络又被称为**全连接神经网络** 

  - **对应位置的元素加权之和，动图**

    ![CNN动图](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/CNN%E5%8A%A8%E5%9B%BE.gif)

  - 在**全连接神经网络**中，如果处理图像，会先将图像的特征（像素点）**转换为一维的数组**。而在**CNN中**，会更直观的在图像**原本的排列**情况下进行计算，如上图所示。所以说CNN很适合**处理图像信息**



#### 1.3什么是过滤器/卷积核？

- 用来与原始图像进行卷积的特殊矩阵，即卷积核似乎和前向传播网络中的每一层的**参数（权重）**类似。只是不再为每个输出神经元配备一组权重。而是**固定一个卷积核权重矩阵**，用卷积的方法去 处理所有输入神经元。所以很直观的可以发现：**CNN的所需要确定的权重序列远远小于全连接神经网络**

  ![卷积核](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%8D%B7%E7%A7%AF%E6%A0%B8.jpg)
- 需要什么样的卷积核？
  - 一些卷积核内部的权重是**特殊构造**的，这些卷积核可以完成一些特殊的特征提取工作。比如前文提到的第一个卷积核就可以提取出垂直的边缘。比如Sobel filter、scharr filter等特殊的卷积核（以前机器学习会用到）
  - 更一般的，正如前向传播一样，可以随机初始化卷积核中的权重。然后用后向传播的方法，**让CNN自己学习能提取一些简单的特征的卷积核**



#### 1.4填充(Padding)

- 在最开始的例子中，输入了6 x 6大小的图片，而卷积核是3 x 3的。最后输出了4 x 4大小的图片
  - 事实上，如果输入$n*n$大小的图片，然后用$f*f$的卷积核卷积。得到的结果会是$(n-f+1)*(n-f+1)$大小的图片
- 上述过程有两个问题
  - 输出的图片与原始图片**大小不一样**！！
  - **边缘的神经元**（像素点）在卷积过程中只被使用过一次，造成的结果就是**原始图像边缘的信息相比中间部分未被充分利用**
- 因此可以用0填充原始图片周围的空间，使输出图片与原始图片大小一样的同时也保证了边缘的像素点被充分卷积。如下图所示：

  ![padding](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/padding.png)
- 如何填充？
  - 如果设填充的维度为p，则要使原始图像和输出图像大小一样，就要满足$n+2p-f+1= n$。所以填充的维度$p=\frac{f-1}2$,对于例子中3 x 3的卷积核，需要填充的维度p = 1
- 必须要填充吗？
  - 事实上有**两种卷积方式**，即Valid Convolutions和Same Convolutions
  - Valid Convolutions，代表不用填充。即$n*n ， f*f\to(n-f+1)*(n-f+1)$
  - Same Convolutions,代表填充使输出与输入图片大小一致。即$(n+2p)*(n+2p)，f*f\to n*n$
- $p=\frac{f-1}2$
  - 可以发现当卷积核的大小f为偶数时，p无法得到整数。这就是为**什么卷积核大小通常取奇数**的一个原因
  - 还有一个可能的原因是，奇数大小的卷积核，**代表中心的像素点只有一个**。这可能更方便计算机视觉的一些算法进行处理



#### 1.5卷积步长（strided convolutions）

- 对于6 x 6的输入和3 x 3的卷积核的例子，每次卷积核移动的步长都是1。但是卷积步长可以为其他数字吗？
- 当然可以，但是改变卷积步长同样会影响到输出的图片大小
- 假设卷积步长stride为s，则输出的图片大小为$\lfloor\frac{n+2p-f}s+1\rfloor * \lfloor\frac{n+2p-f}s+1\rfloor$
- 同时如果要做Same Convolutions，则$\lfloor\frac{n+2p-f}s+1\rfloor=n$



#### 1.6三维卷积（RGB images）

- 在上面的讨论中，只考虑了灰度图像。而生活中，彩色的图像会更普遍。
- 彩色的图片拥有三个色彩通道，即RGB Channel。这使一个RGB彩色图片的维度变为了n x m x 3
  - 通常令输入的图片是一个（width，height，depth(channel)）的三维张量
- 那么如何将CNN应用到彩色的图片上？
  - 首先将卷积核也扩展为3维的，即每个色彩通道都对应一个相同的卷积核。
  - 然后类似切多层蛋糕的方式，将图像的每个通道和通道对应的卷积核进行卷积
  - 最后将卷积得到的**3个输出线性相加**，即得到了RGB图像的卷积输出
- 图例如下

  ![RGB卷积](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/RGB%E5%8D%B7%E7%A7%AF.png)

  ![RGB卷积2](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/RGB%E5%8D%B7%E7%A7%AF2.gif)



#### 1.7多个卷积核（Multiple filters）

- 上面的边缘检测例子中，使用了一个卷积核获取图片的垂直边缘。如果想要获取水平的边缘，就又要使用水平的卷积核再次对图像进行卷积处理
- 如果想同时获取图像的水平和垂直边缘，可以将两个卷积核得到的输出图像叠加起来
- 更进一步，如果将各个角度的卷积核输出的图像组合为一个深度为n的图像，那么可以更好的对图像提取出边缘特征。图示如下：

  ![多个卷积核](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%A4%9A%E4%B8%AA%E5%8D%B7%E7%A7%AF%E6%A0%B8.png)

- *202105：日常使用Pytorch中时Conv2d(in_features = 3,  out_features=64)。其中in_features就代表了原始数据的通道（channel）数量，out_feature代表了卷积核的数量f。tips：数据通过一个卷积层处理后，卷积核有多少个则输出的数据的通道数量就有多少个*



### 2.简单的卷积神经网络的构成

#### 2.1一层卷积层

**符号规定**

- 用$f^{(l)}$表示第$l$和$l-1$层之间卷积核的大小（filter size）
- 用$p^{(l)}$表示第$l$层输入神经核的填充数量（padding）
- 用$s^{(l)}$表示第$l$层的卷积步长（stride）
- 用$n_h^{(l)},n_w^{(l)},n_c^{(l)}$表示第$l$层的输入或输出的图片的高（height）、宽（weight）、通道数（channel or depth）。或第$l$和$l-1$层之间卷积核的形状,同时$n_c^{(l)}$也表示卷积核的数量
- 用$a^{(l)}$表示加上偏执项后用Relu函数激活的第$l-1$层的输出项，第$l$层的输入项
- 设输入图片的形状和输出的形状为Input：$(n_h^{(l-1)},n_w^{(l-1)},n_c^{(l-1)})$，Output:$(n_h^{(l)},n_w^{(l)},n_c^{(l)})$
  - 则输入和输出两层的图片形状关系有：$n_{h\ or\ w}^{(l)}=\lfloor\frac{n_{h\ or\ w}^{(l-1)}+2p^{(l)}-f^{(l)}}{s^{(l)}}+1\rfloor$
  - 中间的每个卷积核的形状为：$(f^{(l)},f^{(l)},n_c^{(l-1)})$,且一共有$n_c^{(l)}$个卷积核
  - 因此需要确定的权重的个数为$f^{(l)}*f^{(l)}*n_c^{(l-1)}*n_c^{(l)}$
  - 需要为每个卷积核设定一个偏执项（bias），因此偏执项的个数=卷积核的个数$n_c^{(l)}$

**一个例子**

![一层CNN的例子](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%80%E5%B1%82CNN%E7%9A%84%E4%BE%8B%E5%AD%90.png)

- 上图中，输入图片的形状为6 x 6 x 3,用两个3 x 3 x 3的卷积核进行卷积
  - 输出两个4 x 4的矩阵
  - 对两个矩阵，先为矩阵中的每个元素加上相同的偏执项b（这在python中可以用广播实现），再使用非线性的激活函数Relu进行激活，得到两个激活后的4x4矩阵
  - 将两个矩阵堆叠起来，作为输出
  - 以上操作，即完成了一层的卷积神经网络

**数学表达**

- 设输入$x = a^{(0)}$,所有卷积核的权重矩阵为$w^{(1)}$,未激活的输出为$z^{(1)}$
- 则$z^{(1)}=w^{(1)}a^{(0)}+b$
- 激活后$a^{(1)}=g(z^{(1)})$

**一些启发**

- 一层卷积层需要确定的权重数量与什么有关？
  - 一层CNN的需要确定的权重数量完全取决于**卷积核的大小和数量**
  - 这与全连接神经网络中**权重由两层间的输入神经元和输出神经元的个数决定**完全不同
  - 反映在输入的特征数量非常大时，全连接神经网络需要确定的权重会很多。而CNN不管输入的特征数量是多还是少，一旦卷积核确定后，需要确定的权重的数量是不会改变的
  - 因此CNN非常适合处理特征数量非常多的数据，这称为CNN的“**避免过拟合**”的特性。
  - *202105：称为**参数共享**可能更准确一点*



#### 2.2池化层（Pooling layer）

**What**？

- 对卷积处理的**输出矩阵**进行的一种特殊操作

- 最常见的Max Polling 就是**取出对应输出矩阵区域中的最大值**

  ![Pooling](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/Pooling.png)

- 如上图，相当于用一个类似f=2，s=2，p=0的卷积核对输出矩阵进行了一种特殊处理。也就是取出对应区域中的最大值

- 相当于对图像进行了下采样（SUbsamping）



**why**？

- 直观理解
  - 在边缘检测中，输出矩阵出现比较大的数值，可能往往代表发现了某个特征或边缘。
  - 因此在CNN中会提取这个特征，也就是保留这个最大值。其他比较小的值就不去关心了
  - 用类似卷积核的窗口扫描整个输出矩阵，可能右上角的区域经过卷积操作出现了某个边缘或特征，保留这个最大值。而左上角没有，就算取这个区域的最大值，这个最大值也不会很大
- 好处
  - 减少模型的规模，同时又不太影响模型的性能
  - 提高计算速度
  - 提高鲁棒性
  - 202105：由于池化层，原始图像相当于被下采样了。因此整个CNN网络中的原始数据产生不同尺寸的特征矩阵，**有利于发现图片中不同尺寸的模式**



**池化层的超参数**

- 类似于卷积核的参数，池化层需要确定对应的超参数形成一个类似于卷积核的窗口以便划分区域，但也仅仅而已。**池化层不需要进行卷积操作，也不需要去优化其中的参数（因为没有）**
- 需要确定的超参数：池化层卷积核的大小f，以及卷积步长s，通常不需要填充（padding）p=0
- 比较常用f=2，s=2的超参数，则数据矩阵经过池化层后大小会变为原输入一半



**池化层的类型**

- Max Polling：取每个区域的最大值，这是最常使用的池化层类型
- Average Polling：取每个区域的平均值



#### 2.3全连接层（FC layer）

- 其实就是之前第五章学过的全连接神经网络
- why？
  - 之前用卷积层和池化层的到的输出矩阵，相当直接在原图片上提取出**二维的特征**
  - 然而为了使用“分类器”的方法，需要将特征空间展开为线性的（一维）
  - 因此为了在一维的特征空间中进行训练，当然要使用全连接神经网络或者全连接层处理CNN网络最后的输出的二维特征矩阵



### 3.整合起来：一套完整的CNN

#### 3.1手写数字识别的例子（LeNet-5网络）

![一个完整的CNN](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84CNN.png)

- 流程
  - 输入一个32 x 32 x 3的手写数字图片
  - 用f=5，s=1，p=0的6个卷积核卷积，输出28 x 28 x 6的矩阵。然后经过f=2，s=2的池化层，输出14 x 14 x 6的矩阵。这样的一个卷积层+池化层视为神经网络中的一层
  - 继续用f=5，s=1的16个卷积核卷积并使用f=2，s=2的池化层，输出5 x 5 x 16的矩阵
  - 展开输出结果，为（400,1）的向量，然后进入全连接层
  - 最后通过softmax层输出（10,1）的向量，进行手写数字的判断
- 流程中的参数变化

  ![CNN中参数变化](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/CNN%E4%B8%AD%E5%8F%82%E6%95%B0%E5%8F%98%E5%8C%96.png)

  - 在进入全连接层之前，$n_h$和$n_h$变得越来越小，而$n_c$变得越来越大。图片尺寸减小，但通道数增加
  - 整个流程中**需要激活的神经元变得越来越少**，但是要注意激活值如果减少的太快，会影响模型的性能
  - 整个流程中，除了输入层和池化层不需要确定参数外，其他层需要确定的参数到全连接层急剧增加
- 流程中的超参数
  - 超参数的确定，暂时建议参考别人论文中成熟的模型的超参数
- 为什么需要卷积？
  - 卷积优于全连接的地方在于**参数共享和稀疏连接**，这就保证了卷积层需要确定的权重远远少于全连接层
    - 参数共享：一个特征提取器（卷积核）可以适用于图片的任意部分
    - 稀疏连接：每层的输出的每个神经元都由一小部分输入神经元（f x f个）决定



#### 3.2 编程实现

具体实现请见github notebook[我在吴恩达的Coursera上的对应作业：Convolutional Neural Networks: Step by Step]()

下面主要说下编程实现的难点

**切片**：

- 具体实现时，需要构造一个滑动窗口在二维数据矩阵上滑动切割一个个数据区域。然后针对这个切片计算卷积

  - 如何切片？

    - 设卷积过后输出矩阵的高为H，宽为W，即输出矩阵A形状为[H，W]。则$A[i][j]$代表原始数据矩阵中一个窗口大小的矩阵经过卷积后的结果。如下图所示，输出中的"0"就是$A[0][1$]代表的数值。

    - 那么输入矩阵M这个对应的区域就是$M[i * stride :  i  * stride + f - 1][j * stride :  j  * stride + f - 1]$，且其大小和卷积核一样。如下图所示，$A[0][1]$是由$M[0 * 1 : 0 * 1 + 3 - 1][1 * 1  : 1 * 1 + 3 - 1]$，即$M[0 : 2][1 : 3]$卷积得到的

    - 因此**切片是个二重循环**：遍历输出矩阵的每个位置$A[i][j]$，由$A[i][j]$反推输入矩阵M对应的区域

    - 最后一般**输入和输出的数据是一批数据然后有多个通道**，即输入矩阵M的形状为（样本数量m，H，W，样本的通道数C）。切片时也要遍历样本数量和通道数量，因此具体实现时这是一个**四重循环**

      ![卷积核](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/卷积核.jpg)

**卷积的后向传播**：

- 局部梯度
  - 卷积前向传播的公式为$f(x) = <W, X> + b = \sum_{i=1}^{n}{w_ix_i} + b$

    - 对X求导，即X的局部梯度：

    $\frac {\partial f}{\partial X}=\frac {\partial {\sum_{i=1}^{n}{w_ix_i} + b}}{\partial X}=[\frac {\partial {\sum_{i=1}^{n}{w_ix_i} + b}}{\partial x_1},\frac {\partial {\sum_{i=1}^{n}{w_ix_i} + b}}{\partial x_2},\frac {\partial {\sum_{i=1}^{n}{w_ix_i} + b}}{\partial x_3}...,\frac {\partial {\sum_{i=1}^{n}{w_ix_i} + b}}{\partial x_n}]^T=[w_1,w_2,w_3...w_n]^T=W$

    - 对W求导同理$\frac{\partial f}{\partial W} = X$
    - 对b求导:$\frac{\partial f}{\partial b} = 1$

- 求梯度：求梯度公式：dW = 上一层的梯度流 * W的局部梯度，其他同理

  - 设上一层(方向由后向前)传过来的梯度流为dZ，其形状"上一层高H，上一层宽W"。
  - 和切片里解释的一样，$dZ[i][j]$和切片区域$M[i * stride :  i  * stride + f - 1][j * stride :  j  * stride + f - 1]$有关，令这个区域的梯度值$dZ[i][j]$为dz。以及令这个区域切片为a，形状“卷积核大小f，f，切片的通道数c”
  - 则：
    - $dW=\sum_{i=0}^c a_c*dz$，这里需要将通道上的梯度累加起来
    - $db=\sum_{i=0}^c 1*dz$
    - $da=\sum_{i=0}^c W_c*dz$

- 注意事项
  - 与切片中的解释一样，具体实现时还要遍历样本数量和样本通道数，因此卷积的后向传播还是4重循环

**padding的恢复：**

- padding的恢复不涉及到梯度，因此也没有padding的后向传播这种说法。这就是一个逆操作
- 设输入矩阵M形状（H，W），padding大小为p。则矩阵形状变为(H+2p, W+2p)。如何恢复？
- 从padding后的Mp矩阵中切出来和原来M一样的区域就行了！**即Mp[p : -p, p : -p] == M**

**pooling的后向传播：**

- 和前面的一样还是对一个切片区域做pooling层的后向传播。但是取矩阵中的最大值和平均值这种操作有导数吗？如果没有导数又该如何更新梯度？

- 这也是后向传播中的特殊情况：当一种操作无法求导时，需要人工定义此时该操作的导数。池化层的反向传播也称为**反池化（UnPooling）**

  - 对于MaxPooling：需要前向传播时，记录**特征矩阵中最大值的位置**。反向传播时规定：此时maxpooling层的梯度矩阵为，反池化输入矩阵对应的数值恢复到之前记录的位置上去，其他未被记录的位置为0。语言比较抽象，具体操作如图所示：

    ![image-20210526135306029](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/image-20210526135306029.png)

  - 对于AveragePooling：无需记录前向传播时的位置信息，只需要**将反池化输入矩阵对应的数值恢复到所有梯度矩阵对应窗口的位置上去**。如上图所示

- 很明显反池化并不是池化操作的完全逆操作，因此**反池化无法完全恢复到池化之前的状态**！！也就是无法用池化层下采样后在使用反池化上采样恢复原来的状态。到至于为什么这样是可行的？我看了几篇博客，感觉还是不清楚。

  注：上采样除了CV常用的双线性采样等采样的方法外，还有反池化、反卷积、转置卷积的方法。在Unet等需要上采样恢复到下采样前的形状的网络中以上方法会被用到。但这些和pooling层的后向传播就没有关系了。

**一部分代码关键：**

```python
# 输入的数据矩阵形状分别为（样本数量m，样本高h，样本宽w，样本通道数量c）
(m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape
# 权重矩阵形状分别为（卷积核大小f，卷积核大小f，样本的通道数量c，卷积核的数量n_c）
(f, f, n_C_prev, n_C) = W.shape

# 根据公式计算目前样本大小和卷积核尺寸下输出矩阵的大小
n_H = int((n_H_prev-f+2*pad)/stride+1)
n_W = int((n_W_prev-f+2*pad)/stride+1)

# Z用来保存卷积后的输出，很明显形状应该是（样本数量m，输出高h，输出宽w，卷积核的数量）
# 所以为了求Z需要4重循环！！！！
Z = np.zeros((m,n_H,n_W,n_C))

# 有pad就padding一下所有样本
A_prev_pad = zero_pad(A_prev,pad)

# 遍历样本
for i in range(m):
    # 用A_prev_pad[i]选出单个样本，shape“1，n_H, n_W, n_C”
    a_prev_pad = A_prev_pad[i]
   
	# 遍历垂直的下标
    for h in range(n_H):
        # 计算垂直边界[vert_start ，vet_end）注意区间开闭
        vert_start = h * stride
        vert_end = h * stride + f
		
        # 遍历水平下标
        for w in range(n_W):      
            # 计算水平边界[horiz_start， horiz_end）注意区间开闭
            horiz_start = w * stride
            horiz_end = w * stride + f
			# 很明显a_prev_pad[vert_start :vet_end[horiz_start： horiz_end]切出了一个区域
            # 这个区域shape"1, f, f, n_C"与卷积核的大小一模一样
            # 那么就可以直接对应位置相乘最后全部相加
            
            # 遍历通道
            for c in range(n_C):
           	 # 1.如果要计算卷积，即前向传播
                # 和卷积核大小一样的切片shape"1, f, f, n_C"
                a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]
                weights = W[:,:,:,c] # 每个通道的权重 shape"f, f, n_prev_c, 1"
                biases = b[0,0,0,c]  # 每个通道的偏执 shape"1,"
                # conv_single_step()实现就是<X, W> + b: 参数和数据矩阵的内积 + 偏执
                Z[i, h, w, c] = conv_single_step(a_slice_prev,weights,biases)
           	 
            # 2.如果要计算卷积的后向传播
                a_slice = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]
                da_prev_pad[vert_start:vert_end, horiz_start:horiz_end,:] += 		                                                      W[:,:,:,c] * dZ[i, h, w, c]
                dW[:,:,:,c] += a_slice * dZ[i, h, w, c]
                db[:,:,:,c] += dZ[i, h, w, c]
# 恢复padding
dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]
            
           	 # 3.如果要计算pooling的前向传播
            	# pooling的切片形状为"1, f, f, 1",即只要切片区域中的最大值或平均值
            	a_prev_slice = A_prev[i,vert_start:vert_end,horiz_start:horiz_end,c]
                if mode == "max":
                  	A[i, h, w, c] = np.max(a_prev_slice)
                elif mode == "average":
                    A[i, h, w, c] = np.average(a_prev_slice)
           
        	# 4.如果要计算pooling的后向传播：
            	# 上层传过来的梯度的一个切片值 shape"1,"
            	da = dA[i,h,w,c]
            	if mode == "max":
                    # 需要反池化的切片，这里只是需要该切片的位置和形状
                    a_prev_slice = a_prev[vert_start:vert_end,horiz_start:horiz_end,c]
                    # create_mask_from_window（）返回该切片中最大元素的掩码矩阵
                	mask = create_mask_from_window(a_prev_slice)
					# 一个梯度值与掩码矩阵相乘，自然是输出与掩码矩阵形状一样的梯度矩阵
                    dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] +=
                    									np.multiply(mask,a)
                        
                elif mode == "average":
                    shape = (f,f)
                    # distribute_value(da,shape)就是将梯度值放到（f,f）的梯度矩阵中的每个位置上去
                    dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] +=
                     									distribute_value(da,shape)
```





### 4.经典网络与残差网络

#### 4.1经典网络

- LeNet-5
- AlexNet
- VCG-16

#### 4.2残差网络（Residual Network/ResNet）

- 直觉告诉我们，神经网络层数越深，则模型的性能也应该更好。但是由于**梯度消失和梯度爆炸**的问题。传统的神经网络在深度增加到一定程度的时候，模型性能反而会下降。但是残差网络的提出可以帮我们解决这个问题
- 实现方法
  - 通过**跳层连接**，可以将某层的激活值传给更深层次的网络进行激活。

**残差块（Residual Block）**

![残差块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E6%AE%8B%E5%B7%AE%E5%9D%97.png)

- 如上图所示，展示了残差网络中的一个基本单元——残差块
  - 按照经典网络中的激活方法，$a^{(l)},a^{(l+1)},a^{(l+2)}$三层的激活由各自的权重、偏执项以及非线性激活函数Relu完成
  - 而残差网络中，运用**跳层连接**。让$a^{(l)}$参与到了$a^{(l+2)}$的激活

**残差网络**

- ![残差网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C.png)
- 多个残差块的加入，使经典网络变成了残差网络
- 残差网络很好的解决了随着神经网络的层次增加到一定的层次后，性能变差的问题。帮助人们构建更深层的网络

**为什么残差网络能够在层数增加的同时保证神经网络的性能**？

![为什么残差网络有用](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E4%B8%BA%E4%BB%80%E4%B9%88%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C%E6%9C%89%E7%94%A8.png)

- 假设已经用一个深层的网络得到了一个激活项$a^{(l)}$,在这个网络的基础上再增加两层，并进行跳层连接构建一个残差块的到新的激活项$a^{(l+2)}$
- 则$a^{(l+2)}=g(w^{(l+2)}a^{(l+1)}+b^{(l+1)}+a^{(l)})$
  - 因为正则化和权重退化的等原因，更深层次的权重和偏执项会渐进于0
  - 因此超级深层的线性激活项也会趋近于0，因此用Relu激活时，激活项趋近0。这表示该层已经很难再训练模型了
  - 然而残差块的加入，让即使深层的激活项已经趋近于0，**也可得到$a^{(l)}=a^{(l+2)}$的结果**
  - 也就是说残**差网络让深层的层次最差也是学习到了使两层相等的方法**，这对神经网络的性能有一个**保底作用**。即残差块的加入让层次增加的同时，神经网络的性能不会下降。当然如果层次增加神经网络的性能也增加了，这更是乐于见到的
  - 换言之：**残差块最差也能学会恒等函数**，这为神经网络的性能兜了底

**注意事项**

- 残差网络因为跳层连接的存在，因此最好要保证卷积操作时Same convolution。才能保证每层特征矩阵的大小相同，这样大小相同的特征矩阵才能直接在通道维度上相加
- 当然如果不保证Same convolution，就需要在跳层连接中增加1x1的卷积核，采用适当的卷积步长和卷积核数量将前面的特征矩阵映射到与后面特征矩阵一样的大小



### 5.其他变种CNN（了解）

#### 5.1 1x1卷积/network in network（了解）

- 对通道或深度为1的数据用1x1卷积核卷积，似乎只能使数据乘上一个常数
- 对通道>=2的矩阵使用1 x 1卷积核
  - 池化层能够缩减原矩阵的高和宽，而1 x 1卷积核能够**减少或增加原矩阵的通道**。比如用20个1 x 1的卷积核卷积20 x 20 x 200的矩阵，可以得到20 x 20 x 20的输出
  - 如果将矩阵中通道一样的所有元素视为一个个神经元，对其做1 x 1卷积操作。则相当于对这些神经元进行了全连接操作。输入原始矩阵通道个数的神经元，输出1 x 1卷积核个数的神经元
- 有什么作用？
  - 缩减信道，构建瓶颈层（bottomneck），缩小计算规模
  - 应用于其他卷积神经网络的框架中，比如下面要提到的inception网络

#### 5.2 Inception网络（了解）

- What？
  - 使用inception模块来帮助人们确定在CNN框架构建中，需要多大的过滤器？需不需要池化层等？
- inception模块

  ![inception模块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E6%A8%A1%E5%9D%97.png)

  - 思想：
    - 如上图，在不确定使用1 x 1、3 x 3还是5 x 5的卷积核和池化层时
    - 可以对原始矩阵做上述的一系列操作。但是要**保证相同卷积**
    - 然后将所有不确定操作得到的输出的通道连接起来，就形成了一个inception模块
    - 让神经网络自己学习需要多大的卷积核以及什么样的池化层
  - 计算量问题
    - 虽然让神经网络自己学习自己的构造，听起来很美好。但是在网络中额外构建这样的inception模块会消耗非常多的计算资源
    - 如何减少计算量？
      - 以上图中5 x 5卷积操作为例，32个5 x 5卷积核卷积（28,28,192）输出（28,28,32）的矩阵需要进行5x5x192x28x28x32大约1.2亿次乘法运算
      - 而先运用16个**1 x 1卷积核**将原矩阵的通道数缩减，在使用32个5x5的卷积核输出（28,28,32）的矩阵。需要进行28x28x16x192+28x28x32x5x5x16大约240万次计算，计算量相对直接计算减少到了10%
      - ![在inception中用1x1卷积](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E5%9C%A8inception%E4%B8%AD%E7%94%A81x1%E5%8D%B7%E7%A7%AF.png)
- inception网络
  - 使用1x1卷积核减少计算量后的inception模块
    - ![inception网络模块](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9D%97.png)
    - 如上图所示，对卷积操作先进行1x1卷积在进行nxn的卷积，而对池化层（相同卷积）操作后再进行1x1卷积都可以缩减信道。达到减少计算量的目的
  - 多个上述inception模块，构成inception网络
    - ![inception网络](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/inception%E7%BD%91%E7%BB%9C.png)



### 6.迁移学习（transfer learning）（了解）

- what？
  - 举个例子，如果已经训练好一个可以分1000个类别的分类器。而现在又需要训练一个可以分100个类别的分类器，该怎么做？
  - 重新训练肯定是最低效的，一个比较合理的方法是，改变之前1000个类别分类器的模型的softmax层，使其输出为100个类别。同时不改变其他层次的权重和参数
  - 这就是迁移学习的核心：仅仅改变已经训练好的网络的一部分，以便适应新的、类似的模型
- 使用迁移学习
  - 当下载别人已经训练好的模型或自己的模型后，根据现在需要新训练的模型而加入的新样本的多少。来确定需要冻结多少层次（即不改变这些层次），以及重新训练多少层次
  - 一般而言，**冻结是从前面的层次开始的**。因为后面层次训练的参数和权重会更贴近于训练的样本的特点，也就是后面层次更加抽象更加符合训练样本的个性。
  - 以及根据输出要求的改变而重新训练对应的softmax层
  - 甚至不改变冻结的层次，而剩下的层次按照新的要求直接重塑其框架，再训练



### 7.数据扩充（data augmentation）（了解）

- 深度学习中，特别在计算机视觉中，训练集的数量十分缺少。因此需要使用数据扩充的方法
- 数据扩充的方法
  - 最简单的：对原始图像进行镜像，随机裁剪，旋转，适度扭曲等操作
  - 色彩转换（color shifting）：对RGB三个通道加上不同的失真值

    ![色彩转换png](https://pic-1305686174.cos.ap-nanjing.myqcloud.com/%E8%89%B2%E5%BD%A9%E8%BD%AC%E6%8D%A2png.png)

    - PCA色彩扩充
  - 结合使用
    - 比如在原始数据图像中取出一部分，先进行镜像，随机裁剪等操作，在进行色彩转换
    - 然后将得到的一批数据集代入神经网络模型中进行训练

202105：修订